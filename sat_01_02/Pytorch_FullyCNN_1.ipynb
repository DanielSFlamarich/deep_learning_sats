{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network in PyTorch\n",
    "[A PyTorch tutorial – deep learning in Python](https://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a simple 4-layer  fully connected neural network (including an “input layer” and two hidden layers) to classify the hand-written digits of the MNIST dataset. The architecture we’ll use can be seen in the figure below:\n",
    "\n",
    "![Model FullyCNN](assets/CNTK-Dense-example-architecture.jpg)\n",
    "\n",
    "The input layer consists of 28 x 28 (=784) greyscale pixels which constitute the input data of the MNIST data set. This input is then passed through two fully connected hidden layers, each with 200 nodes, with the nodes utilizing a [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Finally, we have an output layer with ten nodes corresponding to the 10 possible classes of hand-written digits (i.e. 0 to 9). We will use a [softmax](https://en.wikipedia.org/wiki/Softmax_function) output layer to perform this classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network class\n",
    "\n",
    "In order to create a neural network in PyTorch, you need to use the included class nn.Module. To use this base class, we also need to use Python class inheritance – this basically allows us to use all of the functionality of the nn.Module base class, but still have overwriting capabilities of the base class for the model construction / forward pass through the network. Some actual code will help explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the class definition, you can see the inheritance of the base class nn.Module. Then, in the first line of the class initialization (def __init__(self):) we have the required Python super() function, which creates an instance of the base nn.Module class. The following three lines is where we create our fully connected layers as per the architecture diagram. A fully connected neural network layer is represented by the nn.Linear object, with the first argument in the definition being the number of nodes in layer l and the next argument being the number of nodes in layer l+1. As you can observer, the first layer takes the 28 x 28 input pixels and connects to the first 200 node hidden layer. Then we have another 200 to 200 hidden layer, and finally a connection between the last hidden layer and the output layer (with 10 nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ve setup the “skeleton” of our network architecture, we have to define how data flows through out network. We do this by defining a forward() method in our class – this method overwrites a dummy method in the base class, and needs to be defined for each network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return F.log_softmax(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the forward() method, we supply the input data x as the primary argument. We feed this into our first fully connected layer (self.fc1(x)) and then apply a ReLU activation to the nodes in this layer using F.relu(). Because of the hierarchical nature of this network, we replace x at each stage, feeding it into the next layer. We do this through our three fully connected layers, except for the last one – instead of a ReLU activation we return a log softmax “activation”. This, combined with the negative log likelihood loss function which will be defined later, gives us a multi-class cross entropy based loss function which we will use to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an instance of this network architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "net = Net()\n",
    "print(net)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the instance of the class Net, we get the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Net (\n",
    "(fc1): Linear (784 -> 200)\n",
    "(fc2): Linear (200 -> 200)\n",
    "(fc3): Linear (200 -> 10)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty handy as it confirms the structure of our network for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Next we have to setup an optimizer and a loss criterion:\n",
    "\n",
    "```python\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# create a loss function\n",
    "criterion = nn.NLLLoss()\n",
    "```\n",
    "\n",
    "In the first line, we create a stochastic gradient descent optimizer, and we specify the learning rate (which I’ve passed to this function as 0.01) and a momentum of 0.9. The other ingredient we need to supply to our optimizer is all the parameters of our network – thankfully PyTorch make supplying these parameters easy by the .parameters() method of the base nn.Module class that we inherit from in the Net class.\n",
    "\n",
    "Next, we set our loss criterion to be the negative log likelihood loss – this combined with our log softmax output from the neural network gives us an equivalent cross entropy loss for our 10 classification classes.\n",
    "\n",
    "Now it’s time to train the network. During training, I will be extracting data from a data loader object which is included in the PyTorch utilities module. This data loader will supply batches of input and target data which we’ll supply to our network and loss function respectively. Here’s the full training code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer training loop is the number of epochs, whereas the inner training loop runs through the entire training set in batch sizes which are specified in the code as batch_size. On the next line, we convert data and target into PyTorch variables. The MNIST input data-set which is supplied in the torchvision package (which you’ll need to install using pip if you run the code for this tutorial) has the size (batch_size, 1, 28, 28) when extracted from the data loader – this 4D tensor is more suited to convolutional neural network architecture, and not so much our fully connected network. Therefore we need to flatten out the (1, 28, 28) data to a single dimension of 28 x 28 =  784 input nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines: \n",
    "\n",
    "```python\n",
    "data = data.view(-1, 28*28)\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The .view() function operates on PyTorch variables to reshape them. If we want to be agnostic about the size of a given dimension, we can use the “-1” notation in the size definition. So by using data.view(-1, 28*28) we say that the second dimension must be equal to 28 x 28, but the first dimension should be calculated from the size of the original data variable. In practice, this means that data will now be of size (batch_size, 784). We can pass a batch of input data like this into our network and the magic of PyTorch will do all the hard work by efficiently performing the required operations on the tensors.\n",
    "\n",
    "On the next line, we run optimizer.zero_grad() – this zeroes / resets all the gradients in the model, so that it is ready to go for the next back propagation pass. In other libraries this is performed implicitly, but in PyTorch you have to remember to do it explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s single out the next two lines:\n",
    "\n",
    "```python\n",
    "net_out = net(data)\n",
    "loss = criterion(net_out, target)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line is where we pass the input data batch into the model – this will actually call the forward() method in our Net class. After this line is run, the variable net_out will now hold the log softmax output of our neural network for the given data batch. That’s one of the great things about PyTorch, you can activate whatever normal Python debugger you usually use and instantly get a gauge of what is happening in your network. This is opposed to other deep learning libraries such as TensorFlow and Keras which require elaborate debugging sessions to be setup before you can check out what your network is actually producing.\n",
    "\n",
    "The second line is where we get the negative log likelihood loss between the output of our network and our target batch data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line here runs a back-propagation operation from the loss Variable backwards through the network. If you compare this with our review of the .backward() operation that we undertook earlier in this PyTorch tutorial, you’ll notice that we aren’t supplying the .backward() operation with an argument. Scalar variables, when we call .backward() on them, don’t require arguments – only tensors require a matching sized tensor argument to be passed to the .backward() operation.\n",
    "\n",
    "The next line is where we tell PyTorch to execute a gradient descent step based on the gradients calculated during the .backward() operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print out some results every time we reach a certain number of iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if batch_idx % log_interval == 0:\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This print function shows our progress through the epochs and also gives the network loss at that point in the training. Note how you access the loss – you access the Variable .data property, which in this case will be a single valued array. We access the scalar loss by executing loss.data[0].\n",
    "\n",
    "Running this training loop you’ll get an output that looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "rain Epoch: 9 [52000/60000 (87%)] Loss: 0.015086\n",
    "Train Epoch: 9 [52000/60000 (87%)] Loss: 0.015086\n",
    "Train Epoch: 9 [54000/60000 (90%)] Loss: 0.030631\n",
    "Train Epoch: 9 [56000/60000 (93%)] Loss: 0.052631\n",
    "Train Epoch: 9 [58000/60000 (97%)] Loss: 0.052678\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 epochs, you should get a loss value down around the <0.05 magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the trained network on our test MNIST data set, we can run the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).item()\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop is the same as the previous training loop up until the test_loss line – here we extract the network loss using the .data[0] property as before, but all in the same line. Next, we have the pred line, where the data.max(1) method is used – this .max() method can return the index of the maximum value in a certain dimension of a tensor. Now, the output of our neural network will be of size (batch_size, 10), where each value of the 10-length second dimension is a log probability which the network assigns to each output class (i.e. it is the log probability of whether the given image is a digit between 0 and 9). So for each input sample/row in the batch, net_out.data will look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "[-1.3106e+01, -1.6731e+01, -1.1728e+01, -1.1995e+01, -1.5886e+01, -1.7700e+01, -2.4950e+01, -5.9817e-04, -1.3334e+01, -7.4527e+00]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value with the highest log probability is the digit that the network considers to be the most probable given the input image – this is the best prediction of the class from the network. In the example of net_out.data above, it is the value -5.9817e-04 which is maximum, which corresponds to the digit “7”. So for this sample, the predicted digit is “7”. The .max(1) function will determine this maximum value in the second dimension (if we wanted the maximum in the first dimension, we’d supply an argument of 0) and returns both the maximum value that it has found, and the index that this maximum value was found at. It therefore has a size of (batch_size, 2) – in this case we are interested in the index where the maximum value is found at, therefore we access these values by calling .max(1)[1].\n",
    "\n",
    "Now we have the prediction of the neural network for each sample in the batch determined, we can compare this with the actual target class from our training data, and count how many times in the batch the neural network got it right. We can use the PyTorch .eq() function to do this, which compares the values in two tensors and if they match, returns a 1. If they don’t match, it returns a 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "correct += pred.eq(target.data).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By summing the output of the .eq() function, we get a count of the number of times the neural network has produced a correct output, and we take an accumulating sum of these correct predictions so that we can determine the overall accuracy of the network on our test data set. Finally, after running through the test data in batches, we print out the averaged loss and accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the network for 10 epochs, we get the following output from the above code on the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```text\n",
    "Test set: Average loss: 0.0003, Accuracy: 9783/10000 (98%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 98% accuracy – not bad!\n",
    "\n",
    "So there you have it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.331256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.calderon.machuca/.local/share/virtualenvs/kaka-sPsbq-e8/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 2.208768\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 1.941722\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 1.358670\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 0.792633\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.635027\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.563788\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.441449\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.435862\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.471460\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.382576\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.230259\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.335611\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.308573\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.337263\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.300235\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.275453\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.274764\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.259939\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.306546\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.401564\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.337595\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.273402\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.260285\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.337327\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.314566\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.255633\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.242874\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.192875\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.282085\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.187582\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.224098\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.245035\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.229908\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.246532\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.287876\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.211900\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.157716\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.190987\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.334060\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.195320\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.161536\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.222286\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.190648\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.199728\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.099031\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.149848\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.246627\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.136266\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.139746\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.189327\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.197827\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.237183\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.166352\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.163831\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.140267\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.133894\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.211566\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.177935\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.155141\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.206005\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.167809\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.157777\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.219682\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.137156\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.173808\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.180569\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.093020\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.141173\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.180180\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.131461\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.137893\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.108980\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.132296\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.121518\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.257504\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.143082\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.133140\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.132399\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.180085\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.126750\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.078676\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.115384\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.178024\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.103537\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.161281\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.108246\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.201098\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.134905\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.127219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.calderon.machuca/.local/share/virtualenvs/kaka-sPsbq-e8/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 9622/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def simple_gradient():\n",
    "    # print the gradient of 2x^2 + 5x\n",
    "    x = Variable(torch.ones(2, 2) * 2, requires_grad=True)\n",
    "    z = 2 * (x * x) + 5 * x\n",
    "    # run the backpropagation\n",
    "    z.backward(torch.ones(2, 2))\n",
    "    print(x.grad)\n",
    "\n",
    "\n",
    "def create_nn(batch_size=200, learning_rate=0.01, epochs=3,\n",
    "              log_interval=10):\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(28 * 28, 200)\n",
    "            self.fc2 = nn.Linear(200, 200)\n",
    "            self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.log_softmax(x)\n",
    "\n",
    "    net = Net()\n",
    "    print(net)\n",
    "\n",
    "    # create a stochastic gradient descent optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    # create a loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # run the main training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "            data = data.view(-1, 28*28)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    # run a test loop\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        net_out = net(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(net_out, target).item()\n",
    "        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_opt = 2\n",
    "    if run_opt == 1:\n",
    "        simple_gradient()\n",
    "    elif run_opt == 2:\n",
    "        create_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
